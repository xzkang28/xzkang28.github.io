<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> k近邻算法 · zikang</title><meta name="description" content="机器学习实战-k近邻算法（knn）"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://zikang.me/atom.xml" title="zikang"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">k近邻算法</h1><div class="post-info">Mar 28, 2017</div><div class="post-content"><p>机器学习实战-k近邻算法（knn）</p>
<a id="more"></a>
<p>k近邻算法是一个分类算法，比如我们可以根据电影的打斗镜头和接吻镜头相应的数量来判断电影是动作片还是爱情片。</p>
<p>k近邻算法的工作原理是：存在一个数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本中特征最相似数据（最临近）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。</p>
<p>回到电影分类的例子，假如我有一部电影已知它的打斗镜头和接吻镜头，那我们怎么根据样本集来判断呢？</p>
<table>
<thead>
<tr>
<th>电影名称</th>
<th style="text-align:center">打斗镜头</th>
<th style="text-align:center">接吻镜头</th>
<th style="text-align:center">电影类型</th>
</tr>
</thead>
<tbody>
<tr>
<td>California Man</td>
<td style="text-align:center">3</td>
<td style="text-align:center">104</td>
<td style="text-align:center">爱情片</td>
</tr>
<tr>
<td>He ‘s Not Really into Dudes</td>
<td style="text-align:center">2</td>
<td style="text-align:center">100</td>
<td style="text-align:center">爱情片</td>
</tr>
<tr>
<td>Beautiful Woman</td>
<td style="text-align:center">1</td>
<td style="text-align:center">81</td>
<td style="text-align:center">爱情片</td>
</tr>
<tr>
<td>Kevin Longblade</td>
<td style="text-align:center">101</td>
<td style="text-align:center">10</td>
<td style="text-align:center">动作片</td>
</tr>
<tr>
<td>Robo Slayer</td>
<td style="text-align:center">99</td>
<td style="text-align:center">5</td>
<td style="text-align:center">动作片</td>
</tr>
<tr>
<td>Amped II</td>
<td style="text-align:center">98</td>
<td style="text-align:center">2</td>
<td style="text-align:center">动作片</td>
</tr>
<tr>
<td>?</td>
<td style="text-align:center">18</td>
<td style="text-align:center">90</td>
<td style="text-align:center">未知</td>
</tr>
</tbody>
</table>
<p>我们可以把数据抽象为一个向量（或者看成坐标点），比如California Man的数据就可以抽象成(3,104)；那如何计算两部电影之间的相似性呢？在这里我们可以使用欧式距离公式来计算两个向量点之间的距离：<br>$$<br>d=\sqrt{(xA_0-xB_0)^2+(xA_1-xB_1)^2}<br>$$<br>那未知电影与California Man之间的距离计算为：<br>$$<br>d=\sqrt{(18-3)^2+(90-104)^2}=20.5<br>$$<br>同理，其他电影与未知电影之间的距离也可以计算，最后按距离递增排序：</p>
<table>
<thead>
<tr>
<th>电影名称</th>
<th style="text-align:center">与未知电影的距离</th>
</tr>
</thead>
<tbody>
<tr>
<td>California Man</td>
<td style="text-align:center">20.5</td>
</tr>
<tr>
<td>He ‘s Not Really into Dudes</td>
<td style="text-align:center">18.7</td>
</tr>
<tr>
<td>Beautiful Woman</td>
<td style="text-align:center">19.2</td>
</tr>
<tr>
<td>Kevin Longblade</td>
<td style="text-align:center">115.3</td>
</tr>
<tr>
<td>Robo Slayer</td>
<td style="text-align:center">117.4</td>
</tr>
<tr>
<td>Amped II</td>
<td style="text-align:center">118.9</td>
</tr>
</tbody>
</table>
<p>如果我们假定k=3，则最靠近的电影依次是California Man、He ‘s Not Really into Dudes和Beautiful Woman。k-近邻算法按照距离最接近的三部电影来决定未知电影的类型，而这三部全部都是爱情片，因此我们判断未知电影也是爱情片。</p>
<p>接下来上knn算法的代码代码是python，需要导入numpy第三方包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX,dataSet,labels,k)</span>:</span></div><div class="line">	dataSetSize = dataSet.shape[<span class="number">0</span>]		//取矩阵的列数</div><div class="line">	diffMat = tile(inX,(dataSetSize,<span class="number">1</span>))-dataSet		//建立未知电影的矩阵，并减去样本矩阵</div><div class="line">	sqDiffMat = diffMat**<span class="number">2</span>							</div><div class="line">	sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)				</div><div class="line">	disatances = sqDistances**<span class="number">0.5</span>		//计算欧式距离</div><div class="line">	sortedDistIndicies = disatances.argsort()		//递增排序</div><div class="line">	classCount=&#123;&#125;</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(k):		//对前k个数据的类别计数，返回出现次数最多的类别</div><div class="line">		voteIlabel = labels[sortedDistIndicies[i]]</div><div class="line">		classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>)+<span class="number">1</span></div><div class="line">	sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="keyword">True</span>)</div><div class="line">	<span class="keyword">print</span> classCount,sortedClassCount</div><div class="line">	<span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>classify0() 函数有4个输入参数：用于分类的输入向量是inX，输入的训练样本集为dataSet，标签向量为labels（爱情片抽象为1，动作片抽象为0），最后的参数k表示用于选择最近邻居的数目，其中标签向量的元素数目和矩阵dataSet的行数相同。</p>
<p>为了更好理解，我把例子中的输入的参数也贴出来：<br>$$<br>inX=\left[\begin{matrix}<br>        18 &amp; 90 \<br>        \end{matrix}\right]<br>$$</p>
<p>$$<br>dataSet=\left[\begin{matrix}<br>        3 &amp; 104 \\<br>        2 &amp; 100 \\<br>        1 &amp; 81 \\<br>        101 &amp; 10 \\<br>        99 &amp; 5 \\<br>        98 &amp; 2 \\<br>        \end{matrix}\right]<br>$$</p>
<p>$$<br>labels=\left[\begin{matrix}<br>        1 \\<br>        1 \\<br>        1 \\<br>        0 \\<br>        0 \\<br>        0 \\<br>        \end{matrix}\right]<br>$$</p>
<p>我们已经成功使用k-近邻算法构造了一个分类器，想想看分类器一定是正确的吗？答案是否定的，分类器并不会得到百分百正确的结果，因为分类器还会受到多种因素的影响，比如数据集和k值的变化就可能产生不同的结果；我们可以多种方法来检测分类器的准确率。</p>
<p>接下来我们用一个实际的案例来演示完整的分类器，图像识别是机器学习领域的一个典型应用，其本质原理是把图像转化为电脑能够识别的二进制文件，再通过复杂的分类算法来识别图像，在这里我们先尝试用一个简化版数据集来实践，假设我们有一批处理后的样本集，其中一个文件如下，它的分类是数字3：</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170328/222305843.png" alt="mark"></p>
<p>那么如何去识别一个新的数字呢？接下来上代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span>		//根据样本创建一个<span class="number">1</span>行<span class="number">1024</span>列的训练矩阵</div><div class="line">	returnVect = zeros((<span class="number">1</span>,<span class="number">1024</span>))</div><div class="line">	fr = open(filename)</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">		lineStr = fr.readline()</div><div class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">			returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j] = int(lineStr[j])</div><div class="line">	<span class="keyword">return</span> returnVect</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span>						</div><div class="line">	hwLabels = []</div><div class="line">	traningFileList = listdir(<span class="string">'trainingDigits'</span>)		//根据样本创建一个m行<span class="number">1024</span>列的训练矩阵</div><div class="line">	m = len(traningFileList)</div><div class="line">	traningMat = zeros((m,<span class="number">1024</span>))</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(m):		//根据文件名解析出分类数字</div><div class="line">		fileNameStr = traningFileList[i]</div><div class="line">		fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">		classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</div><div class="line">		hwLabels.append(classNumStr)</div><div class="line">		traningMat[i,:] = img2vector(<span class="string">'trainingDigits/%s'</span>%fileNameStr)</div><div class="line">	testFileList = listdir(<span class="string">'testDigits'</span>)</div><div class="line">	errorCount = <span class="number">0.0</span></div><div class="line">	mTest = len(testFileList)</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):		//用测试集来计算分类器的准确率</div><div class="line">		fileNameStr = testFileList[i]</div><div class="line">		fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">		classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</div><div class="line">		vectorUnderTest = img2vector(<span class="string">'testDigits/%s'</span>%fileNameStr)</div><div class="line">		classifierResult = classify0(vectorUnderTest,traningMat,hwLabels,<span class="number">3</span>)</div><div class="line">		<span class="keyword">print</span><span class="string">"the classfifier came back with: %d,the real answer is :%d"</span>%(classifierResult,classNumStr)</div><div class="line">		<span class="keyword">if</span>(classifierResult != classNumStr):errorCount += <span class="number">1.0</span></div><div class="line">	<span class="keyword">print</span> <span class="string">"\nthe total number of errors is: %d"</span>%errorCount</div><div class="line">	<span class="keyword">print</span> <span class="string">"\nthe total error rate is %f"</span>%(errorCount/float(mTest))</div></pre></td></tr></table></figure>
<p>因为数据集是txt文件，每一个文件名格式都是 3_7.txt，其中3是该样本的实际分类，7是该样本在3分类下的id，分类器先把trainingDigits目录下的所有样本集转成1024维的训练矩阵，再把testDigits目录下的所有测试集也转成1024维的矩阵，再用之前的classify0() 函数来分类，最后再对比分类器分类结果和实际分类，以此来计算分类器的准确率。</p>
<p>最后总结一下，knn近邻算法就像是“近朱者赤，近墨者黑”，如果一部电影的打斗镜头和接吻镜头与另外k部接近，knn近邻算法就会判断这部电影属于另外k部电影中占多数的类型。</p>
<p><a href="http://download.csdn.net/detail/u012491566/6474803" target="_blank" rel="external">本文的数据集和代码下载&gt;&gt;</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/03/30/20170330-决策树算法/" class="prev">上一篇</a><a href="/2017/01/05/20170105-游戏背后的数据分析/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'seansun';
var disqus_identifier = '2017/03/28/20170328-k近邻算法/';
var disqus_title = 'k近邻算法';
var disqus_url = 'http://zikang.me/2017/03/28/20170328-k近邻算法/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//seansun.disqus.com/count.js" async></script><div class="copyright"><p>© 2017 - 2018 <a href="http://zikang.me">子康</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-96130216-2",'auto');ga('send','pageview');</script></body></html>