<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 决策树算法 · zikang</title><meta name="description" content="机器学习实战-决策树算法（ID3）"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://zikang.me/atom.xml" title="zikang"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">决策树算法</h1><div class="post-info">Mar 30, 2017</div><div class="post-content"><p>机器学习实战-决策树算法（ID3）</p>
<a id="more"></a>
<p>决策数也是一种常见的分类方法，比如我们可以粗略地根据一种海洋生物是否能不浮出水面生存，是否有脚蹼来判断它是不是鱼类。</p>
<p>假设我们有一些新的数据样本，现在要对“是否属于鱼类”进行决策，那么我们通常会进行一些“子决策”：先看“不浮出水面是否可以生存”，如果可以生存，再看“是否有脚蹼”，最后得出最终决策：这属于鱼类。显然，决策过程的最终结论对应了我们所希望的判定结果“是”或者“不是”鱼类；决策过程中提出的每个判定问题都是对某个属性的“测试”，例如“不浮出水面是否可以生存？”；每个测试结果或是导出最终结论或是导出进一步的判定问题，其考虑范围是在上次决策结果的限定范围之内，例如“不浮出水面能生存”之后再判断“是否有脚蹼？“。</p>
<table>
<thead>
<tr>
<th style="text-align:center">样本id</th>
<th style="text-align:center">不浮出水面是否可以生存</th>
<th style="text-align:center">是否有脚蹼</th>
<th style="text-align:center">属于鱼类</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">是</td>
<td style="text-align:center">否</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">否</td>
<td style="text-align:center">是</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">否</td>
<td style="text-align:center">是</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
<p>决策过程如下：</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170329/222735012.png" alt="mark"></p>
<p>决策树的原理：一颗决策树包含一个根节点、若个干内部节点和若干个叶节点；叶节点对应属性结果，其他每个节点则对应于一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点包含样本全集，从根节点到每个叶节点的路径对应了一个判定测试序列。决策树学习的目的是为了产生一颗泛化能力强的，即处理未见示例能力强的决策树，其基本流程遵循简单的“分而治之”策略。</p>
<p>之前所说的knn算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解。</p>
<p>再进一步解释决策树算法之前，先介绍俩个概念：”信息熵“（也叫“香农熵”）和”信息增益“：</p>
<p>”信息熵“是度量样本集合纯度最常用的一种指标，假定当前样本集合D中第k类样本所占的比例为$p_k(k=1,2,…,n)$，则D的信息熵定义为：</p>
<p>$$<br>Ent(D)=-\sum_{k=1}^np_klog_2p_k<br>$$<br>$Ent(D)$的值越小，则D的纯度越高。</p>
<p>假定离散属性a有V个可能的取值{$a^1,a^2,…,a^V$},若使用a来对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$。我们可以根据信息熵计算公式计算出$Ent(D^v)$。再考虑到不同的分支节点所包含的样本数不同，给分支节点赋予权重$|D^v|/|D|$，即样本数越多的分支节点影响越大，于是可以计算出属性a对样本集D进行划分所得的“信息增益”<br>$$<br>Gain(D,a)=Ent(D)-\sum_{v=1}^V{|D^v|\over|D|}Ent(D^v)<br>$$<br>一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大，因此，我们可以用信息增益来进行决策树的划分属性选择。</p>
<p>用鱼分类的数据为例，显然，分类只有“鱼类”和“非鱼类”两种，即k=2，而“鱼类”占比2/5，“非鱼类”占比3/5，于是可以求出根节点的信息熵为</p>
<p>$$<br>Ent(D)=-\sum_{k=1}^2p_klog_2p_k=-({2\over5}log_2{2\over5}+{3\over5}log_2{3\over5})=0.29<br>$$<br>然后我们假设以属性“不浮出水面是否能够生存”划分数据集，划分后得到两个子集$D_1{1,2,3}$和$D_2{4,5}$ ；再分别计算$D_1$和$D_2$的信息熵<br>$$<br>Ent(D_1)=-({2\over3}log_2{2\over3}+{1\over3}log_2{1\over3})=0.27<br>$$</p>
<p>$$<br>Ent(D_2)=-({2\over2}log_2{2\over2})=0<br>$$</p>
<p>于是可以计算出属性“不浮出水面是否能够生存”的信息增益为<br>$$<br>\begin{align}<br>Gain(D,不浮出水面是否能够生存)&amp;=Ent(D)-\sum_{v=1}^2{|D^v|\over|D|}Ent(D^v)\\<br>&amp;=0.29-({3\over5}\times0.27+{2\over5}\times0)=0.128<br>\end{align}<br>$$<br>同理我们也可以计算出属性“是否有脚蹼”的信息增益为<br>$$<br>Gain(D,是否有脚蹼)=0.24<br>$$<br>显然，属性“不浮出水面是否能够生存”的信息增益最大，于是它被选为划分属性。这种以信息增益作为划分准则的算法就是著名的ID3决策树算法，接下来贴完整的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span>	//计算信息熵</span><br><span class="line">	numEntries = len(dataSet)</span><br><span class="line">	labelCounts = &#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">		currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">		<span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">			labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">		labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">	shannonEnt = <span class="number">0.0</span></span><br><span class="line">	<span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">		prob = float(labelCounts[key])/numEntries</span><br><span class="line">		shannonEnt -= prob * log(prob,<span class="number">2</span>)</span><br><span class="line">	<span class="keyword">return</span> shannonEnt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet,axis,value)</span>:</span>	//按照给定特征划分数据集</span><br><span class="line">	retDataSet = []</span><br><span class="line">	<span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">		<span class="keyword">if</span> featVec[axis] == value:</span><br><span class="line">			reducedFeatVec = featVec[:axis]</span><br><span class="line">			reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">			retDataSet.append(reducedFeatVec)</span><br><span class="line">	<span class="keyword">return</span> retDataSet</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span>	//通过比较信息增益选择最佳划分属性</span><br><span class="line">	numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">	baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">	bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">		featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">		uniqueVals = set(featList)</span><br><span class="line">		newEntropy = <span class="number">0.0</span></span><br><span class="line">		<span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">			subDataSet = splitDataSet(dataSet,i,value)</span><br><span class="line">			prob = len(subDataSet)/float(len(dataSet))</span><br><span class="line">			newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">		infoGain = baseEntropy - newEntropy</span><br><span class="line">		<span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">			bestInfoGain = infoGain</span><br><span class="line">			bestFeature = i</span><br><span class="line">	<span class="keyword">return</span> bestFeature</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span>	//选择出现次数最多的分类</span><br><span class="line">	classCount = &#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">		<span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.key():classCount[vote] = <span class="number">0</span></span><br><span class="line">		classCount[vote] += <span class="number">1</span></span><br><span class="line">	sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="keyword">True</span>)</span><br><span class="line">	<span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span>	//创建决策树</span><br><span class="line">	classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">	<span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">		<span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">	<span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">		<span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">	bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">	bestFeatLabel = labels[bestFeat]</span><br><span class="line">	myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">	<span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">	featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">	uniqueVals = set(featValues)</span><br><span class="line">	<span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">		subLabels = labels[:]</span><br><span class="line">		myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels)</span><br><span class="line">	<span class="keyword">return</span> myTree</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span>	//使用决策树执行分类</span><br><span class="line">	firstStr = inputTree.keys()[<span class="number">0</span>]</span><br><span class="line">	secondDict = inputTree[firstStr]</span><br><span class="line">	featIndex = featLabels.index(firstStr)</span><br><span class="line">	<span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">		<span class="keyword">if</span> testVec[featIndex] == key:</span><br><span class="line">			<span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:</span><br><span class="line">				classLabel = classify(secondDict[key],featLabels,testVec)</span><br><span class="line">			<span class="keyword">else</span>:</span><br><span class="line">				classLabel = secondDict[key]</span><br><span class="line">	<span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure>
<p>可以看出，ID3决策树算法是一个递归过程，而在算法种有三种情况会导致递归返回：</p>
<ol>
<li><p>当前节点包含的样本全属于同一类别，无需划分。</p>
</li>
<li><p>属性集为空，或是所有样本在所有属性上的取值相同，无法划分。</p>
</li>
<li><p>当前节点包含的样本集合为空，不能划分。</p>
</li>
</ol>
<p>在第2种情况下，我们把当前节点标记为叶节点，并将其类别设定为该节点样本最多的类别；在第3种情况下，同样把当前节点标记为叶节点，但将其类别设定为其父节点所含样本最多的类别，注意这两种情形的处理实质不同：情形2是在利用当前节点的后验分布，而情形3则是把父节点的样本分布作为当前节点的先验分布。</p>
<p>总结一下：决策树算法就是通过信息增益不断递归寻找最好的划分方式，构建树的过程就好比“开枝散叶”，当有新的分类任务时可以很快的从树的“根”到“叶”找到最终的分类结果。</p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/08/05/20180805-《数据化管理》读书笔记/" class="prev">PREV</a><a href="/2017/03/28/20170328-k近邻算法/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'seansun';
var disqus_identifier = '2017/03/30/20170330-决策树算法/';
var disqus_title = '决策树算法';
var disqus_url = 'http://zikang.me/2017/03/30/20170330-决策树算法/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//seansun.disqus.com/count.js" async></script><div class="copyright"><p>© 2017 - 2018 <a href="http://zikang.me">子康</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-96130216-2",'auto');ga('send','pageview');</script></body></html>