<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-03-30T15:47:18.489Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>决策树算法</title>
    <link href="http://yoursite.com/2017/03/30/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%9520170330/"/>
    <id>http://yoursite.com/2017/03/30/决策树算法20170330/</id>
    <published>2017-03-30T15:47:13.000Z</published>
    <updated>2017-03-30T15:47:18.489Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习实战-决策树算法（ID3）</p>
<a id="more"></a>
<p>决策数也是一种常见的分类方法，比如我们可以粗略地根据一种海洋生物是否能不浮出水面生存，是否有脚蹼来判断它是不是鱼类。</p>
<p>假设我们有一些新的数据样本，现在要对“是否属于鱼类”进行决策，那么我们通常会进行一些“子决策”：先看“不浮出水面是否可以生存”，如果可以生存，再看“是否有脚蹼”，最后得出最终决策：这属于鱼类。显然，决策过程的最终结论对应了我们所希望的判定结果“是”或者“不是”鱼类；决策过程中提出的每个判定问题都是对某个属性的“测试”，例如“不浮出水面是否可以生存？”；每个测试结果或是导出最终结论或是导出进一步的判定问题，其考虑范围是在上次决策结果的限定范围之内，例如“不浮出水面能生存”之后再判断“是否有脚蹼？“。</p>
<table>
<thead>
<tr>
<th style="text-align:center">样本id</th>
<th style="text-align:center">不浮出水面是否可以生存</th>
<th style="text-align:center">是否有脚蹼</th>
<th style="text-align:center">属于鱼类</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">是</td>
<td style="text-align:center">否</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">否</td>
<td style="text-align:center">是</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">否</td>
<td style="text-align:center">是</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
<p>决策过程如下：</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170329/222735012.png" alt="mark"></p>
<p>决策树的原理：一颗决策树包含一个根节点、若个干内部节点和若干个叶节点；叶节点对应属性结果，其他每个节点则对应于一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点包含样本全集，从根节点到每个叶节点的路径对应了一个判定测试序列。决策树学习的目的是为了产生一颗泛化能力强的，即处理未见示例能力强的决策树，其基本流程遵循简单的“分而治之”策略。</p>
<p>之前所说的knn算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解。</p>
<p>再进一步解释决策树算法之前，先介绍俩个概念：”信息熵“（也叫“香农熵”）和”信息增益“：</p>
<p>”信息熵“是度量样本集合纯度最常用的一种指标，假定当前样本集合D中第k类样本所占的比例为$p_k(k=1,2,…,n)$，则D的信息熵定义为：</p>
<p>$$<br>Ent(D)=-\sum_{k=1}^np_klog_2p_k<br>$$<br>$Ent(D)$的值越小，则D的纯度越高。</p>
<p>假定离散属性a有V个可能的取值{$a^1,a^2,…,a^V$},若使用a来对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$。我们可以根据信息熵计算公式计算出$Ent(D^v)$。再考虑到不同的分支节点所包含的样本数不同，给分支节点赋予权重$|D^v|/|D|$，即样本数越多的分支节点影响越大，于是可以计算出属性a对样本集D进行划分所得的“信息增益”<br>$$<br>Gain(D,a)=Ent(D)-\sum_{v=1}^V{|D^v|\over|D|}Ent(D^v)<br>$$<br>一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大，因此，我们可以用信息增益来进行决策树的划分属性选择。</p>
<p>用鱼分类的数据为例，显然，分类只有“鱼类”和“非鱼类”两种，即k=2，而“鱼类”占比2/5，“非鱼类”占比3/5，于是可以求出根节点的信息熵为</p>
<p>$$<br>Ent(D)=-\sum_{k=1}^2p_klog_2p_k=-({2\over5}log_2{2\over5}+{3\over5}log_2{3\over5})=0.29<br>$$<br>然后我们假设以属性“不浮出水面是否能够生存”划分数据集，划分后得到两个子集$D_1{1,2,3}$和$D_2{4,5}$ ；再分别计算$D_1$和$D_2$的信息熵<br>$$<br>Ent(D_1)=-({2\over3}log_2{2\over3}+{1\over3}log_2{1\over3})=0.27<br>$$</p>
<p>$$<br>Ent(D_2)=-({2\over2}log_2{2\over2})=0<br>$$</p>
<p>于是可以计算出属性“不浮出水面是否能够生存”的信息增益为<br>$$<br>\begin{align}<br>Gain(D,不浮出水面是否能够生存)&amp;=Ent(D)-\sum_{v=1}^2{|D^v|\over|D|}Ent(D^v)\\<br>&amp;=0.29-({3\over5}\times0.27+{2\over5}\times0)=0.128<br>\end{align}<br>$$<br>同理我们也可以计算出属性“是否有脚蹼”的信息增益为<br>$$<br>Gain(D,是否有脚蹼)=0.24<br>$$<br>显然，属性“不浮出水面是否能够生存”的信息增益最大，于是它被选为划分属性。这种以信息增益作为划分准则的算法就是著名的ID3决策树算法，接下来贴完整的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span>	//计算信息熵</div><div class="line">	numEntries = len(dataSet)</div><div class="line">	labelCounts = &#123;&#125;</div><div class="line">	<span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</div><div class="line">		currentLabel = featVec[<span class="number">-1</span>]</div><div class="line">		<span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</div><div class="line">			labelCounts[currentLabel] = <span class="number">0</span></div><div class="line">		labelCounts[currentLabel] += <span class="number">1</span></div><div class="line">	shannonEnt = <span class="number">0.0</span></div><div class="line">	<span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</div><div class="line">		prob = float(labelCounts[key])/numEntries</div><div class="line">		shannonEnt -= prob * log(prob,<span class="number">2</span>)</div><div class="line">	<span class="keyword">return</span> shannonEnt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet,axis,value)</span>:</span>	//按照给定特征划分数据集</div><div class="line">	retDataSet = []</div><div class="line">	<span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</div><div class="line">		<span class="keyword">if</span> featVec[axis] == value:</div><div class="line">			reducedFeatVec = featVec[:axis]</div><div class="line">			reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</div><div class="line">			retDataSet.append(reducedFeatVec)</div><div class="line">	<span class="keyword">return</span> retDataSet</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span>	//通过比较信息增益选择最佳划分属性</div><div class="line">	numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span></div><div class="line">	baseEntropy = calcShannonEnt(dataSet)</div><div class="line">	bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span></div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</div><div class="line">		featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">		uniqueVals = set(featList)</div><div class="line">		newEntropy = <span class="number">0.0</span></div><div class="line">		<span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">			subDataSet = splitDataSet(dataSet,i,value)</div><div class="line">			prob = len(subDataSet)/float(len(dataSet))</div><div class="line">			newEntropy += prob * calcShannonEnt(subDataSet)</div><div class="line">		infoGain = baseEntropy - newEntropy</div><div class="line">		<span class="keyword">if</span> (infoGain &gt; bestInfoGain):</div><div class="line">			bestInfoGain = infoGain</div><div class="line">			bestFeature = i</div><div class="line">	<span class="keyword">return</span> bestFeature</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span>	//选择出现次数最多的分类</div><div class="line">	classCount = &#123;&#125;</div><div class="line">	<span class="keyword">for</span> vote <span class="keyword">in</span> classList:</div><div class="line">		<span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.key():classCount[vote] = <span class="number">0</span></div><div class="line">		classCount[vote] += <span class="number">1</span></div><div class="line">	sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="keyword">True</span>)</div><div class="line">	<span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span>	//创建决策树</div><div class="line">	classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">	<span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</div><div class="line">		<span class="keyword">return</span> classList[<span class="number">0</span>]</div><div class="line">	<span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</div><div class="line">		<span class="keyword">return</span> majorityCnt(classList)</div><div class="line">	bestFeat = chooseBestFeatureToSplit(dataSet)</div><div class="line">	bestFeatLabel = labels[bestFeat]</div><div class="line">	myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</div><div class="line">	<span class="keyword">del</span>(labels[bestFeat])</div><div class="line">	featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">	uniqueVals = set(featValues)</div><div class="line">	<span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">		subLabels = labels[:]</div><div class="line">		myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels)</div><div class="line">	<span class="keyword">return</span> myTree</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span>	//使用决策树执行分类</div><div class="line">	firstStr = inputTree.keys()[<span class="number">0</span>]</div><div class="line">	secondDict = inputTree[firstStr]</div><div class="line">	featIndex = featLabels.index(firstStr)</div><div class="line">	<span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</div><div class="line">		<span class="keyword">if</span> testVec[featIndex] == key:</div><div class="line">			<span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:</div><div class="line">				classLabel = classify(secondDict[key],featLabels,testVec)</div><div class="line">			<span class="keyword">else</span>:</div><div class="line">				classLabel = secondDict[key]</div><div class="line">	<span class="keyword">return</span> classLabel</div></pre></td></tr></table></figure>
<p>可以看出，ID3决策树算法是一个递归过程，而在算法种有三种情况会导致递归返回：</p>
<ol>
<li><p>当前节点包含的样本全属于同一类别，无需划分。</p>
</li>
<li><p>属性集为空，或是所有样本在所有属性上的取值相同，无法划分。</p>
</li>
<li><p>当前节点包含的样本集合为空，不能划分。</p>
</li>
</ol>
<p>在第2种情况下，我们把当前节点标记为叶节点，并将其类别设定为该节点样本最多的类别；在第3种情况下，同样把当前节点标记为叶节点，但将其类别设定为其父节点所含样本最多的类别，注意这两种情形的处理实质不同：情形2是在利用当前节点的后验分布，而情形3则是把父节点的样本分布作为当前节点的先验分布。</p>
<p>总结一下：决策树算法就是通过信息增益不断递归寻找最好的划分方式，构建树的过程就好比“开枝散叶”，当有新的分类任务时可以很快的从树的“根”到“叶”找到最终的分类结果。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习实战-决策树算法（ID3）&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>k近邻算法</title>
    <link href="http://yoursite.com/2017/03/28/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%9520170328/"/>
    <id>http://yoursite.com/2017/03/28/k近邻算法20170328/</id>
    <published>2017-03-28T15:49:13.000Z</published>
    <updated>2017-03-28T15:34:59.148Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习实战-k近邻算法（knn）</p>
<a id="more"></a>
<p>k近邻算法是一个分类算法，比如我们可以根据电影的打斗镜头和接吻镜头相应的数量来判断电影是动作片还是爱情片。</p>
<p>k近邻算法的工作原理是：存在一个数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本中特征最相似数据（最临近）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。</p>
<p>回到电影分类的例子，假如我有一部电影已知它的打斗镜头和接吻镜头，那我们怎么根据样本集来判断呢？</p>
<table>
<thead>
<tr>
<th>电影名称</th>
<th style="text-align:center">打斗镜头</th>
<th style="text-align:center">接吻镜头</th>
<th style="text-align:center">电影类型</th>
</tr>
</thead>
<tbody>
<tr>
<td>California Man</td>
<td style="text-align:center">3</td>
<td style="text-align:center">104</td>
<td style="text-align:center">爱情片</td>
</tr>
<tr>
<td>He ‘s Not Really into Dudes</td>
<td style="text-align:center">2</td>
<td style="text-align:center">100</td>
<td style="text-align:center">爱情片</td>
</tr>
<tr>
<td>Beautiful Woman</td>
<td style="text-align:center">1</td>
<td style="text-align:center">81</td>
<td style="text-align:center">爱情片</td>
</tr>
<tr>
<td>Kevin Longblade</td>
<td style="text-align:center">101</td>
<td style="text-align:center">10</td>
<td style="text-align:center">动作片</td>
</tr>
<tr>
<td>Robo Slayer</td>
<td style="text-align:center">99</td>
<td style="text-align:center">5</td>
<td style="text-align:center">动作片</td>
</tr>
<tr>
<td>Amped II</td>
<td style="text-align:center">98</td>
<td style="text-align:center">2</td>
<td style="text-align:center">动作片</td>
</tr>
<tr>
<td>?</td>
<td style="text-align:center">18</td>
<td style="text-align:center">90</td>
<td style="text-align:center">未知</td>
</tr>
</tbody>
</table>
<p>我们可以把数据抽象为一个向量（或者看成坐标点），比如California Man的数据就可以抽象成(3,104)；那如何计算两部电影之间的相似性呢？在这里我们可以使用欧式距离公式来计算两个向量点之间的距离：<br>$$<br>d=\sqrt{(xA_0-xB_0)^2+(xA_1-xB_1)^2}<br>$$<br>那未知电影与California Man之间的距离计算为：<br>$$<br>d=\sqrt{(18-3)^2+(90-104)^2}=20.5<br>$$<br>同理，其他电影与未知电影之间的距离也可以计算，最后按距离递增排序：</p>
<table>
<thead>
<tr>
<th>电影名称</th>
<th style="text-align:center">与未知电影的距离</th>
</tr>
</thead>
<tbody>
<tr>
<td>California Man</td>
<td style="text-align:center">20.5</td>
</tr>
<tr>
<td>He ‘s Not Really into Dudes</td>
<td style="text-align:center">18.7</td>
</tr>
<tr>
<td>Beautiful Woman</td>
<td style="text-align:center">19.2</td>
</tr>
<tr>
<td>Kevin Longblade</td>
<td style="text-align:center">115.3</td>
</tr>
<tr>
<td>Robo Slayer</td>
<td style="text-align:center">117.4</td>
</tr>
<tr>
<td>Amped II</td>
<td style="text-align:center">118.9</td>
</tr>
</tbody>
</table>
<p>如果我们假定k=3，则最靠近的电影依次是California Man、He ‘s Not Really into Dudes和Beautiful Woman。k-近邻算法按照距离最接近的三部电影来决定未知电影的类型，而这三部全部都是爱情片，因此我们判断未知电影也是爱情片。</p>
<p>接下来上knn算法的代码代码是python，需要导入numpy第三方包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX,dataSet,labels,k)</span>:</span></div><div class="line">	dataSetSize = dataSet.shape[<span class="number">0</span>]		//取矩阵的列数</div><div class="line">	diffMat = tile(inX,(dataSetSize,<span class="number">1</span>))-dataSet		//建立未知电影的矩阵，并减去样本矩阵</div><div class="line">	sqDiffMat = diffMat**<span class="number">2</span>							</div><div class="line">	sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)				</div><div class="line">	disatances = sqDistances**<span class="number">0.5</span>		//计算欧式距离</div><div class="line">	sortedDistIndicies = disatances.argsort()		//递增排序</div><div class="line">	classCount=&#123;&#125;</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(k):		//对前k个数据的类别计数，返回出现次数最多的类别</div><div class="line">		voteIlabel = labels[sortedDistIndicies[i]]</div><div class="line">		classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>)+<span class="number">1</span></div><div class="line">	sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="keyword">True</span>)</div><div class="line">	<span class="keyword">print</span> classCount,sortedClassCount</div><div class="line">	<span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>classify0() 函数有4个输入参数：用于分类的输入向量是inX，输入的训练样本集为dataSet，标签向量为labels（爱情片抽象为1，动作片抽象为0），最后的参数k表示用于选择最近邻居的数目，其中标签向量的元素数目和矩阵dataSet的行数相同。</p>
<p>为了更好理解，我把例子中的输入的参数也贴出来：<br>$$<br>inX=\left[\begin{matrix}<br>        18 &amp; 90 \<br>        \end{matrix}\right]<br>$$</p>
<p>$$<br>dataSet=\left[\begin{matrix}<br>        3 &amp; 104 \\<br>        2 &amp; 100 \\<br>        1 &amp; 81 \\<br>        101 &amp; 10 \\<br>        99 &amp; 5 \\<br>        98 &amp; 2 \\<br>        \end{matrix}\right]<br>$$</p>
<p>$$<br>labels=\left[\begin{matrix}<br>        1 \\<br>        1 \\<br>        1 \\<br>        0 \\<br>        0 \\<br>        0 \\<br>        \end{matrix}\right]<br>$$</p>
<p>我们已经成功使用k-近邻算法构造了一个分类器，想想看分类器一定是正确的吗？答案是否定的，分类器并不会得到百分百正确的结果，因为分类器还会受到多种因素的影响，比如数据集和k值的变化就可能产生不同的结果；我们可以多种方法来检测分类器的准确率。</p>
<p>接下来我们用一个实际的案例来演示完整的分类器，图像识别是机器学习领域的一个典型应用，其本质原理是把图像转化为电脑能够识别的二进制文件，再通过复杂的分类算法来识别图像，在这里我们先尝试用一个简化版数据集来实践，假设我们有一批处理后的样本集，其中一个文件如下，它的分类是数字3：</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170328/222305843.png" alt="mark"></p>
<p>那么如何去识别一个新的数字呢？接下来上代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span>		//根据样本创建一个<span class="number">1</span>行<span class="number">1024</span>列的训练矩阵</div><div class="line">	returnVect = zeros((<span class="number">1</span>,<span class="number">1024</span>))</div><div class="line">	fr = open(filename)</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">		lineStr = fr.readline()</div><div class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">			returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j] = int(lineStr[j])</div><div class="line">	<span class="keyword">return</span> returnVect</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span>						</div><div class="line">	hwLabels = []</div><div class="line">	traningFileList = listdir(<span class="string">'trainingDigits'</span>)		//根据样本创建一个m行<span class="number">1024</span>列的训练矩阵</div><div class="line">	m = len(traningFileList)</div><div class="line">	traningMat = zeros((m,<span class="number">1024</span>))</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(m):		//根据文件名解析出分类数字</div><div class="line">		fileNameStr = traningFileList[i]</div><div class="line">		fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">		classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</div><div class="line">		hwLabels.append(classNumStr)</div><div class="line">		traningMat[i,:] = img2vector(<span class="string">'trainingDigits/%s'</span>%fileNameStr)</div><div class="line">	testFileList = listdir(<span class="string">'testDigits'</span>)</div><div class="line">	errorCount = <span class="number">0.0</span></div><div class="line">	mTest = len(testFileList)</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):		//用测试集来计算分类器的准确率</div><div class="line">		fileNameStr = testFileList[i]</div><div class="line">		fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">		classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</div><div class="line">		vectorUnderTest = img2vector(<span class="string">'testDigits/%s'</span>%fileNameStr)</div><div class="line">		classifierResult = classify0(vectorUnderTest,traningMat,hwLabels,<span class="number">3</span>)</div><div class="line">		<span class="keyword">print</span><span class="string">"the classfifier came back with: %d,the real answer is :%d"</span>%(classifierResult,classNumStr)</div><div class="line">		<span class="keyword">if</span>(classifierResult != classNumStr):errorCount += <span class="number">1.0</span></div><div class="line">	<span class="keyword">print</span> <span class="string">"\nthe total number of errors is: %d"</span>%errorCount</div><div class="line">	<span class="keyword">print</span> <span class="string">"\nthe total error rate is %f"</span>%(errorCount/float(mTest))</div></pre></td></tr></table></figure>
<p>因为数据集是txt文件，每一个文件名格式都是 3_7.txt，其中3是该样本的实际分类，7是该样本在3分类下的id，分类器先把trainingDigits目录下的所有样本集转成1024维的训练矩阵，再把testDigits目录下的所有测试集也转成1024维的矩阵，再用之前的classify0() 函数来分类，最后再对比分类器分类结果和实际分类，以此来计算分类器的准确率。</p>
<p>最后总结一下，knn近邻算法就像是“近朱者赤，近墨者黑”，如果一部电影的打斗镜头和接吻镜头与另外k部接近，knn近邻算法就会判断这部电影属于另外k部电影中占多数的类型。</p>
<p><a href="http://download.csdn.net/detail/u012491566/6474803" target="_blank" rel="external">本文的数据集和代码下载&gt;&gt;</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习实战-k近邻算法（knn）&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/03/27/hello-world/"/>
    <id>http://yoursite.com/2017/03/27/hello-world/</id>
    <published>2017-03-27T15:16:55.932Z</published>
    <updated>2017-03-27T15:16:55.932Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>O2O创业模版，家政行业转型实例</title>
    <link href="http://yoursite.com/2016/04/28/O2O%E5%88%9B%E4%B8%9A%E6%A8%A1%E7%89%88%EF%BC%8C%E5%AE%B6%E6%94%BF%E8%A1%8C%E4%B8%9A%E8%BD%AC%E5%9E%8B%E5%AE%9E%E4%BE%8B/"/>
    <id>http://yoursite.com/2016/04/28/O2O创业模版，家政行业转型实例/</id>
    <published>2016-04-28T15:47:13.000Z</published>
    <updated>2017-04-04T09:03:32.184Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a>
<h2 id="传统家政行业经营模式"><a href="#传统家政行业经营模式" class="headerlink" title="传统家政行业经营模式"></a>传统家政行业经营模式</h2><p>国内家政服务企业的运营模式主要集中表现为三类组织形式，即中介型家政服务企业、员工制家政服务企业和会员制家政服务企业。中介型的家政服务企业比较常见，运作模式存在历史也比较长；员工制家政服务企业则属于精品型家政服务组织该运作模式，该企业的结构管理较为规范，在团队建设方面少而精良，企业组建投入少、风险小无需大规模、大设施即可获得高收益。会员制家政服务企业是介于中介型家政服务组织和员工制家政服务企业两种模式之间的一种经营管理模式。根据不同经济收入的雇主对家政服务员的需求利用市场经济手段对雇主的不同服务需求而采取不同的服务、管理方法。</p>
<p>O2O模式兴起O2O主要有两种解释片是Online to Offline（线上到线下）。典型应用场景有用户在线上购买或预定服务然后再到线下商户实地享受服务，或者用户在线上购买或预定商品然后再到线下的实体店取货或体验。二是Offline to Online（线下到线上）。应用场景有：用户通过线下实体店体验并选好商品然后通过线上下单来预定商品。目前市场上比较火热的像58到家、阿姨帮、e家洁、小马管家、云家政等等都是基于O2O模式发展的，用户基于位置在平台上下单，服务人员上门服务，之后在线完成支付和点评，从而形成了交易的闭环。</p>
<h2 id="O2O家政的优势分析"><a href="#O2O家政的优势分析" class="headerlink" title="O2O家政的优势分析"></a>O2O家政的优势分析</h2><ol>
<li>顾客评价公开O2O家政公司以每名员工为单位，每次服务之后顾客都会给予其雇佣的员工以相应的评价，这些评价就跟在淘宝上购物是一样的，对所有顾客公开。这样顾客便可以了解每个员工的优缺点，进而选择最适合自己的员工。</li>
<li>价格透明传统家政公司的顾客只能通过家政公司的推荐，了解几个员工的价格，无法了解所有员工的价格，无法进行比较选择。而O2O家政公司公开所有服务人员的雇佣价格，给顾客以最大的自主选择权和知情权。</li>
<li>先工作，后付款O2O家政公司预约不需缴纳任何费用，员工工资在服务期满一个月，或服务结束后，由雇主当面直接付给员工。</li>
<li>不收中介费传统家政公司的在员工工资的基础上，抬高价格，赚取中介费，以维持公司的运营。而O2O家政公司由于公司为网络经营模式，所以运营成本很低。而且工资结算是由顾客直接付给员工，公司并不经手，所以免去了中介费，使顾客得到更大的实惠。</li>
</ol>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170404/165858524.jpg" alt="mark"></p>
<h2 id="O2O家政的劣势分析"><a href="#O2O家政的劣势分析" class="headerlink" title="O2O家政的劣势分析"></a>O2O家政的劣势分析</h2><p>1、社会诚信体系不完善家政服务不同其他传统服务，是入室服务。住宅被法律视为个人隐私范畴，未经房屋主人的同意是不允许外人随意进入，不论在古代还是当今，私闯民宅都是违法的事。因此这个行业除了要求服务者的服务技能良好，还需要建立雇佣双方的信任关系。然而不论哪个行业都会存在因为私利而出现有损行业发展的恶意行径，包括家政行业尤为谨慎。</p>
<p>在社会诚信体系尚不完善的情况下，陌生人之间的信任关系显得尤为脆弱。简单的把家政服务员的信息罗列到网上是不能够解决雇佣双方的信任屏障，尤其对居家保姆这样的特殊职业，雇佣双方更是一个相互选择的过程。雇主希望找到一个放心又能干的阿姨，阿姨则希望能碰到一个慈善又懂得体谅的雇主。因此在诚信体系尚不完善的大环境下，在没有更多可以将双方信息透明化时，双方面谈，消除顾虑是必经的流程。但是短时间的面谈并不能完全取得彼此信任，如果在工作过程中出现问题，雇佣双方仍然需要继续建立信任或者面临重新选择，这是一个高成本的试错过程。</p>
<p>2、行业标准化推行难度大家政行业目前遇到的另一个问题是服务不够标准化，不同企业、不同家政服务员提供的服务质量参差不齐，使得这一行业认可度偏低，用户没有第三方权威渠道获取家政服务员在服务技能上拥有令人信得过的成绩。因此一些家政企业正在试图着力打造自己的品牌，强化自有培训组织建设，希望能够树立行业典范。但目前家政行业还没有出现国家级家政标准化示范企业，因此这会产生在实际执行标准化服务推广过程中出现没有统一规范的现象；其次家政服务员普遍素质不高，接受程度低，这些人的目标不是严格要求自己践行标准化服务，而更多是出于用户的评价影响自己后续接单，实际用户自己也不知道所谓标准化到底是怎样一个内容。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>基于互联网思维的O2O提供的是用户获得服务更畅通的一种渠道，互联网简化了传统的信息传递路径，同时可通过信息化工具对客户群体进行有效的沟通与客户关系管理。信息化提高了家政服务的透明度，迫使服务更加透明化、标准化、品牌化，很大程度上加速了家政服务行业标准化与职业化的进度。客户通过家政企业泛布在线上各个渠道的精准信息，快速获得服务，将家政O2O引人C2C的对话与消费模式，真正从渠道上“消灭中介”，由家政O2O企业提供的标准化培训与服务跟踪及评价体系，改善之前行业散乱的形象。</p>
<p>O2O的线上服务注重去中心化、去平台化、大数据化、生态圈化，但是家政服务终究还是要落实到具体的人对人去完成，因此，要想利用好家政O2O这把利器，必须清晰地认识到并解决家政符文过程中的标准化等若干问题。家政O2O的关键问题在于线下服务标准化和服务质量控制的问题，标准化、规模化、品牌化和体系化既是国内家政行业的发展的主题诉求，更是家政O2O的第一道核心保障。</p>
]]></content>
    
    <summary type="html">
    
      &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;传统家政行业经营模式&quot;&gt;&lt;a href=&quot;#传统家政行业经营模式&quot; class=&quot;headerlink&quot; title=&quot;传统家政行业经营模式&quot;&gt;&lt;/a&gt;传统家政行业经营模式&lt;/h2&gt;&lt;p&gt;国内家政服务企业的运营模式主要集中表现为
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>浅析微信朋友圈广告</title>
    <link href="http://yoursite.com/2016/03/28/%E6%B5%85%E6%9E%90%E5%BE%AE%E4%BF%A1%E6%9C%8B%E5%8F%8B%E5%9C%88%E5%B9%BF%E5%91%8A/"/>
    <id>http://yoursite.com/2016/03/28/浅析微信朋友圈广告/</id>
    <published>2016-03-28T15:47:13.000Z</published>
    <updated>2017-04-04T08:55:15.776Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a>
<h2 id="朋友圈广告的背景"><a href="#朋友圈广告的背景" class="headerlink" title="朋友圈广告的背景"></a>朋友圈广告的背景</h2><p>据统计，至2014年底，微信用户数量达11.2亿人，每月活跃用户4.4亿，而在日常使用中，76.4%的用户会使用朋友圈来查看朋友动态或进行分享，也就是说，每月有3.4亿活跃用户在刷朋友圈，可见微信的朋友圈广告市场非常庞大。</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170404/163125184.png" alt="mark"></p>
<p>朋友圈广告的来历微信朋友圈并不是像微博那样的“广场围观式”社交平台，而是一个基于天然的通讯录联系人形成的“强关系”的“熟人”社交平台。“熟人”社交并非没有先例，国外社交网站Facebook也用过相同的做法。相比于国外成熟的法治社会，中国人还是更愿意相信熟人，微信依靠朋友圈做推广，其作用要大于Facebook和Twitter等国外网站。人们会觉得收到来自熟人的消息，是一则已经进行过一层过滤的广告，可信度比较大。</p>
<p>微信朋友圈广告采用的是Feed信息流广告，与弹出式广告不同，为了避免打扰到用户的体验，信息流广告试图通过收集到的用户信息、历史记录、社交关系和地理位置进行有针对性的投放，力求与用户“想要的”一致。形式类似于朋友的原创内容，基于对微信用户画像进行定向的同时，再通过实时社交混排算法，依托关系链进行互动传播。信息流广告第一次被大面积应用，是在国外社交网站Twitter上。2011年7月，Twitter推出了Prompted Tweets，广告以1/10到1/20的频率出现在普通推文当中，展示形式与普通推文类似，加上了“promoted”字眼。</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170404/163201599.jpg" alt="mark"></p>
<p>随后，Facebook也开始进行信息流广告的投放。在2012年上市之前，Facebook推出了为移动设备设计的广告平台，在news-feed信息流中植入广告。Facebook内部采用一个指标“Engagement”来评价一个内容是否会吸引当前用户，它决定一个内容是否出现在用户的news-feed中，以及出现在什么位置。Engagement这个指数是通过以下几个互动行为的加权公式算出的：喜欢、分享、点击、评论，至于这四个维度经过什么样的加权公式来计算，这是Facebook内部最大的秘密，而且这个加权公式是动态的，随时可能调整。</p>
<p>目前，Facebook在全球有13.5亿用户，其中移动用户占11.2亿，在2014年第三季度，移动端广告为Facebook带来的收入是19.5亿美元，占整体收入的66%，而信息流广告已成为了Facebook最大的收入来源和主要的盈利模式。</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170404/163214784.png" alt="mark"></p>
<h3 id="朋友圈广告的特点"><a href="#朋友圈广告的特点" class="headerlink" title="朋友圈广告的特点"></a>朋友圈广告的特点</h3><p>与Facebook和Twitter的信息流广告类似，微信的朋友圈广告也有如下的特点：</p>
<ol>
<li>微信朋友圈广告采用头像加名称的表现方式和朋友圈信息外观基本一致，点击后可以关注。</li>
<li>广告右上角有一个推广标签，点击标签可以选择“不感兴趣”从而关闭此广告。</li>
<li>广告中有详情链接，点击链接可以进入广告页面，但没有转发功能。</li>
<li>具有互动性，用户可以点赞也可以随意发表评论；互动越多，广告对用户好友展示的概率越大，如果没有互动，6个小时后广告会自动消失。</li>
<li>广告的投放有效时间为7天，对于单个用户来说，每48小时内只会看到一则广告。</li>
</ol>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170404/163227087.png" alt="mark"></p>
<h3 id="朋友圈广告的优劣势"><a href="#朋友圈广告的优劣势" class="headerlink" title="朋友圈广告的优劣势"></a>朋友圈广告的优劣势</h3><p>朋友圈广告的优势在于“熟人社交”所带来的信任感。好友在朋友圈广告里点赞或者评论，用户都可以看到。而好友的点赞或者评论很有可能会影响用户的购买决策，如果朋友圈大部分好友都给予了广告评论，那么用户看到朋友圈这则广告的概率就会更高，通过这种方式投放广告，拉近了广告主与受众的心理距离，能够达到很好的传播效果。</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170404/163239373.png" alt="mark"></p>
<p>但是随着朋友圈的扩张，陌生的面孔是逐渐增多的，朋友圈成员之间原本的强联系正渐渐变弱，疏离感加强，传播效果不免会打折扣。从首批广告的传播效果来看，人们不知不觉已经成为了广告主口碑营销的工具。然而，如果微信未来的一些做法引起一部分用户的反感，一旦社交平台上的广告没能引发大规模的群体参与热情，或者在内容沟通创意上没有找到足够引起共鸣的点，新鲜一过大家也就审美疲劳回归常态。这类广告形式最适宜那些想要快速建立品牌关注度的广告主，以付费式的社交媒体操作对于品牌有极大的助力。微信信息流广告给品牌提供了更多付费社交的广告机会，这对于增强品牌知名度和传播高质量内容来说是非常有利的途径。</p>
<p>此外，能否最大程度地来精准投放也是摆在微信面前的课题。有效的信息流广告要能为消费者提供真正需要而且不必过多去考虑并选择的产品。显然，这也许会发生在将来，但还不是现在。从微信朋友圈广告公布的定向标准来看，涵盖用户的年龄、性别、地域、操作系统、网络状况和用户兴趣，这些标准与新浪微博和人人网的定向标准并无太大差异。虽然目前微信的用户总数已经达到可提供海量的累积数据，但微信仍然尚未对庞大数据进行系统性的标签化管理，故目前还无法达到真正精准的投放，这也解释了为何第一批微信广告引来了用户激烈的讨论，因为很多用户收到的信息流广告与品牌锁定用户在人群归属上是有一定程度的落差的。</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170404/163248745.png" alt="mark"></p>
<p>不管怎么说，微信朋友圈广告市场是很庞大的，除了腾讯自身，做为普通的广告主和广告公司如何利用朋友圈做精准营销也是值得仔细研究的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;朋友圈广告的背景&quot;&gt;&lt;a href=&quot;#朋友圈广告的背景&quot; class=&quot;headerlink&quot; title=&quot;朋友圈广告的背景&quot;&gt;&lt;/a&gt;朋友圈广告的背景&lt;/h2&gt;&lt;p&gt;据统计，至2014年底，微信用户数量达11.2亿人，每月
    
    </summary>
    
    
  </entry>
  
</feed>
