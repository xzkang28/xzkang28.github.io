<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-03-28T15:34:59.148Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k近邻算法</title>
    <link href="http://yoursite.com/2017/03/28/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%9520170328/"/>
    <id>http://yoursite.com/2017/03/28/k近邻算法20170328/</id>
    <published>2017-03-28T15:49:13.000Z</published>
    <updated>2017-03-28T15:34:59.148Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习实战-k近邻算法（knn）</p>
<a id="more"></a>
<p>k近邻算法是一个分类算法，比如我们可以根据电影的打斗镜头和接吻镜头相应的数量来判断电影是动作片还是爱情片。</p>
<p>k近邻算法的工作原理是：存在一个数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本中特征最相似数据（最临近）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。</p>
<p>回到电影分类的例子，假如我有一部电影已知它的打斗镜头和接吻镜头，那我们怎么根据样本集来判断呢？</p>
<table>
<thead>
<tr>
<th>电影名称</th>
<th style="text-align:center">打斗镜头</th>
<th style="text-align:center">接吻镜头</th>
<th style="text-align:center">电影类型</th>
</tr>
</thead>
<tbody>
<tr>
<td>California Man</td>
<td style="text-align:center">3</td>
<td style="text-align:center">104</td>
<td style="text-align:center">爱情片</td>
</tr>
<tr>
<td>He ‘s Not Really into Dudes</td>
<td style="text-align:center">2</td>
<td style="text-align:center">100</td>
<td style="text-align:center">爱情片</td>
</tr>
<tr>
<td>Beautiful Woman</td>
<td style="text-align:center">1</td>
<td style="text-align:center">81</td>
<td style="text-align:center">爱情片</td>
</tr>
<tr>
<td>Kevin Longblade</td>
<td style="text-align:center">101</td>
<td style="text-align:center">10</td>
<td style="text-align:center">动作片</td>
</tr>
<tr>
<td>Robo Slayer</td>
<td style="text-align:center">99</td>
<td style="text-align:center">5</td>
<td style="text-align:center">动作片</td>
</tr>
<tr>
<td>Amped II</td>
<td style="text-align:center">98</td>
<td style="text-align:center">2</td>
<td style="text-align:center">动作片</td>
</tr>
<tr>
<td>?</td>
<td style="text-align:center">18</td>
<td style="text-align:center">90</td>
<td style="text-align:center">未知</td>
</tr>
</tbody>
</table>
<p>我们可以把数据抽象为一个向量（或者看成坐标点），比如California Man的数据就可以抽象成(3,104)；那如何计算两部电影之间的相似性呢？在这里我们可以使用欧式距离公式来计算两个向量点之间的距离：<br>$$<br>d=\sqrt{(xA_0-xB_0)^2+(xA_1-xB_1)^2}<br>$$<br>那未知电影与California Man之间的距离计算为：<br>$$<br>d=\sqrt{(18-3)^2+(90-104)^2}=20.5<br>$$<br>同理，其他电影与未知电影之间的距离也可以计算，最后按距离递增排序：</p>
<table>
<thead>
<tr>
<th>电影名称</th>
<th style="text-align:center">与未知电影的距离</th>
</tr>
</thead>
<tbody>
<tr>
<td>California Man</td>
<td style="text-align:center">20.5</td>
</tr>
<tr>
<td>He ‘s Not Really into Dudes</td>
<td style="text-align:center">18.7</td>
</tr>
<tr>
<td>Beautiful Woman</td>
<td style="text-align:center">19.2</td>
</tr>
<tr>
<td>Kevin Longblade</td>
<td style="text-align:center">115.3</td>
</tr>
<tr>
<td>Robo Slayer</td>
<td style="text-align:center">117.4</td>
</tr>
<tr>
<td>Amped II</td>
<td style="text-align:center">118.9</td>
</tr>
</tbody>
</table>
<p>如果我们假定k=3，则最靠近的电影依次是California Man、He ‘s Not Really into Dudes和Beautiful Woman。k-近邻算法按照距离最接近的三部电影来决定未知电影的类型，而这三部全部都是爱情片，因此我们判断未知电影也是爱情片。</p>
<p>接下来上knn算法的代码代码是python，需要导入numpy第三方包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX,dataSet,labels,k)</span>:</span></div><div class="line">	dataSetSize = dataSet.shape[<span class="number">0</span>]		//取矩阵的列数</div><div class="line">	diffMat = tile(inX,(dataSetSize,<span class="number">1</span>))-dataSet		//建立未知电影的矩阵，并减去样本矩阵</div><div class="line">	sqDiffMat = diffMat**<span class="number">2</span>							</div><div class="line">	sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)				</div><div class="line">	disatances = sqDistances**<span class="number">0.5</span>		//计算欧式距离</div><div class="line">	sortedDistIndicies = disatances.argsort()		//递增排序</div><div class="line">	classCount=&#123;&#125;</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(k):		//对前k个数据的类别计数，返回出现次数最多的类别</div><div class="line">		voteIlabel = labels[sortedDistIndicies[i]]</div><div class="line">		classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>)+<span class="number">1</span></div><div class="line">	sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="keyword">True</span>)</div><div class="line">	<span class="keyword">print</span> classCount,sortedClassCount</div><div class="line">	<span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>classify0() 函数有4个输入参数：用于分类的输入向量是inX，输入的训练样本集为dataSet，标签向量为labels（爱情片抽象为1，动作片抽象为0），最后的参数k表示用于选择最近邻居的数目，其中标签向量的元素数目和矩阵dataSet的行数相同。</p>
<p>为了更好理解，我把例子中的输入的参数也贴出来：<br>$$<br>inX=\left[\begin{matrix}<br>        18 &amp; 90 \<br>        \end{matrix}\right]<br>$$</p>
<p>$$<br>dataSet=\left[\begin{matrix}<br>        3 &amp; 104 \\<br>        2 &amp; 100 \\<br>        1 &amp; 81 \\<br>        101 &amp; 10 \\<br>        99 &amp; 5 \\<br>        98 &amp; 2 \\<br>        \end{matrix}\right]<br>$$</p>
<p>$$<br>labels=\left[\begin{matrix}<br>        1 \\<br>        1 \\<br>        1 \\<br>        0 \\<br>        0 \\<br>        0 \\<br>        \end{matrix}\right]<br>$$</p>
<p>我们已经成功使用k-近邻算法构造了一个分类器，想想看分类器一定是正确的吗？答案是否定的，分类器并不会得到百分百正确的结果，因为分类器还会受到多种因素的影响，比如数据集和k值的变化就可能产生不同的结果；我们可以多种方法来检测分类器的准确率。</p>
<p>接下来我们用一个实际的案例来演示完整的分类器，图像识别是机器学习领域的一个典型应用，其本质原理是把图像转化为电脑能够识别的二进制文件，再通过复杂的分类算法来识别图像，在这里我们先尝试用一个简化版数据集来实践，假设我们有一批处理后的样本集，其中一个文件如下，它的分类是数字3：</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170328/222305843.png" alt="mark"></p>
<p>那么如何去识别一个新的数字呢？接下来上代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span>		//根据样本创建一个<span class="number">1</span>行<span class="number">1024</span>列的训练矩阵</div><div class="line">	returnVect = zeros((<span class="number">1</span>,<span class="number">1024</span>))</div><div class="line">	fr = open(filename)</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">		lineStr = fr.readline()</div><div class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">			returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j] = int(lineStr[j])</div><div class="line">	<span class="keyword">return</span> returnVect</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span>						</div><div class="line">	hwLabels = []</div><div class="line">	traningFileList = listdir(<span class="string">'trainingDigits'</span>)		//根据样本创建一个m行<span class="number">1024</span>列的训练矩阵</div><div class="line">	m = len(traningFileList)</div><div class="line">	traningMat = zeros((m,<span class="number">1024</span>))</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(m):		//根据文件名解析出分类数字</div><div class="line">		fileNameStr = traningFileList[i]</div><div class="line">		fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">		classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</div><div class="line">		hwLabels.append(classNumStr)</div><div class="line">		traningMat[i,:] = img2vector(<span class="string">'trainingDigits/%s'</span>%fileNameStr)</div><div class="line">	testFileList = listdir(<span class="string">'testDigits'</span>)</div><div class="line">	errorCount = <span class="number">0.0</span></div><div class="line">	mTest = len(testFileList)</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):		//用测试集来计算分类器的准确率</div><div class="line">		fileNameStr = testFileList[i]</div><div class="line">		fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">		classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</div><div class="line">		vectorUnderTest = img2vector(<span class="string">'testDigits/%s'</span>%fileNameStr)</div><div class="line">		classifierResult = classify0(vectorUnderTest,traningMat,hwLabels,<span class="number">3</span>)</div><div class="line">		<span class="keyword">print</span><span class="string">"the classfifier came back with: %d,the real answer is :%d"</span>%(classifierResult,classNumStr)</div><div class="line">		<span class="keyword">if</span>(classifierResult != classNumStr):errorCount += <span class="number">1.0</span></div><div class="line">	<span class="keyword">print</span> <span class="string">"\nthe total number of errors is: %d"</span>%errorCount</div><div class="line">	<span class="keyword">print</span> <span class="string">"\nthe total error rate is %f"</span>%(errorCount/float(mTest))</div></pre></td></tr></table></figure>
<p>因为数据集是txt文件，每一个文件名格式都是 3_7.txt，其中3是该样本的实际分类，7是该样本在3分类下的id，分类器先把trainingDigits目录下的所有样本集转成1024维的训练矩阵，再把testDigits目录下的所有测试集也转成1024维的矩阵，再用之前的classify0() 函数来分类，最后再对比分类器分类结果和实际分类，以此来计算分类器的准确率。</p>
<p>最后总结一下，knn近邻算法就像是“近朱者赤，近墨者黑”，如果一部电影的打斗镜头和接吻镜头与另外k部接近，knn近邻算法就会判断这部电影属于另外k部电影中占多数的类型。</p>
<p><a href="http://download.csdn.net/detail/u012491566/6474803" target="_blank" rel="external">本文的数据集和代码下载&gt;&gt;</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习实战-k近邻算法（knn）&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>决策树算法</title>
    <link href="http://yoursite.com/2017/03/28/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%9520170330/"/>
    <id>http://yoursite.com/2017/03/28/决策树算法20170330/</id>
    <published>2017-03-28T15:49:13.000Z</published>
    <updated>2017-03-30T15:44:47.694Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习实战-决策树算法（ID3）</p>
<a id="more"></a>
<p>决策数也是一种常见的分类方法，比如我们可以粗略地根据一种海洋生物是否能不浮出水面生存，是否有脚蹼来判断它是不是鱼类。</p>
<p>假设我们有一些新的数据样本，现在要对“是否属于鱼类”进行决策，那么我们通常会进行一些“子决策”：先看“不浮出水面是否可以生存”，如果可以生存，再看“是否有脚蹼”，最后得出最终决策：这属于鱼类。显然，决策过程的最终结论对应了我们所希望的判定结果“是”或者“不是”鱼类；决策过程中提出的每个判定问题都是对某个属性的“测试”，例如“不浮出水面是否可以生存？”；每个测试结果或是导出最终结论或是导出进一步的判定问题，其考虑范围是在上次决策结果的限定范围之内，例如“不浮出水面能生存”之后再判断“是否有脚蹼？“。</p>
<table>
<thead>
<tr>
<th style="text-align:center">样本id</th>
<th style="text-align:center">不浮出水面是否可以生存</th>
<th style="text-align:center">是否有脚蹼</th>
<th style="text-align:center">属于鱼类</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">是</td>
<td style="text-align:center">否</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">否</td>
<td style="text-align:center">是</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">否</td>
<td style="text-align:center">是</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
<p>决策过程如下：</p>
<p><img src="http://om2zpy4xm.bkt.clouddn.com/blog/20170329/222735012.png" alt="mark"></p>
<p>决策树的原理：一颗决策树包含一个根节点、若个干内部节点和若干个叶节点；叶节点对应属性结果，其他每个节点则对应于一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点包含样本全集，从根节点到每个叶节点的路径对应了一个判定测试序列。决策树学习的目的是为了产生一颗泛化能力强的，即处理未见示例能力强的决策树，其基本流程遵循简单的“分而治之”策略。</p>
<p>之前所说的knn算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解。</p>
<p>再进一步解释决策树算法之前，先介绍俩个概念：”信息熵“（也叫“香农熵”）和”信息增益“：</p>
<p>”信息熵“是度量样本集合纯度最常用的一种指标，假定当前样本集合D中第k类样本所占的比例为$p_k(k=1,2,…,n)$，则D的信息熵定义为：</p>
<p>$$<br>Ent(D)=-\sum_{k=1}^np_klog_2p_k<br>$$<br>$Ent(D)$的值越小，则D的纯度越高。</p>
<p>假定离散属性a有V个可能的取值{$a^1,a^2,…,a^V$},若使用a来对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$。我们可以根据信息熵计算公式计算出$Ent(D^v)$。再考虑到不同的分支节点所包含的样本数不同，给分支节点赋予权重$|D^v|/|D|$，即样本数越多的分支节点影响越大，于是可以计算出属性a对样本集D进行划分所得的“信息增益”<br>$$<br>Gain(D,a)=Ent(D)-\sum_{v=1}^V{|D^v|\over|D|}Ent(D^v)<br>$$<br>一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大，因此，我们可以用信息增益来进行决策树的划分属性选择。</p>
<p>用鱼分类的数据为例，显然，分类只有“鱼类”和“非鱼类”两种，即k=2，而“鱼类”占比2/5，“非鱼类”占比3/5，于是可以求出根节点的信息熵为</p>
<p>$$<br>Ent(D)=-\sum_{k=1}^2p_klog_2p_k=-({2\over5}log_2{2\over5}+{3\over5}log_2{3\over5})=0.29<br>$$<br>然后我们假设以属性“不浮出水面是否能够生存”划分数据集，划分后得到两个子集$D_1{1,2,3}$和$D_2{4,5}$ ；再分别计算$D_1$和$D_2$的信息熵<br>$$<br>Ent(D_1)=-({2\over3}log_2{2\over3}+{1\over3}log_2{1\over3})=0.27<br>$$</p>
<p>$$<br>Ent(D_2)=-({2\over2}log_2{2\over2})=0<br>$$</p>
<p>于是可以计算出属性“不浮出水面是否能够生存”的信息增益为<br>$$<br>\begin{align}<br>Gain(D,不浮出水面是否能够生存)&amp;=Ent(D)-\sum_{v=1}^2{|D^v|\over|D|}Ent(D^v)\<br>&amp;=0.29-({3\over5}\times0.27+{2\over5}\times0)=0.128<br>\end{align}<br>$$<br>同理我们也可以计算出属性“是否有脚蹼”的信息增益为<br>$$<br>Gain(D,是否有脚蹼)=0.24<br>$$<br>显然，属性“不浮出水面是否能够生存”的信息增益最大，于是它被选为划分属性。这种以信息增益作为划分准则的算法就是著名的ID3决策树算法，接下来贴完整的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span>	//计算信息熵</div><div class="line">	numEntries = len(dataSet)</div><div class="line">	labelCounts = &#123;&#125;</div><div class="line">	<span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</div><div class="line">		currentLabel = featVec[<span class="number">-1</span>]</div><div class="line">		<span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</div><div class="line">			labelCounts[currentLabel] = <span class="number">0</span></div><div class="line">		labelCounts[currentLabel] += <span class="number">1</span></div><div class="line">	shannonEnt = <span class="number">0.0</span></div><div class="line">	<span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</div><div class="line">		prob = float(labelCounts[key])/numEntries</div><div class="line">		shannonEnt -= prob * log(prob,<span class="number">2</span>)</div><div class="line">	<span class="keyword">return</span> shannonEnt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet,axis,value)</span>:</span>	//按照给定特征划分数据集</div><div class="line">	retDataSet = []</div><div class="line">	<span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</div><div class="line">		<span class="keyword">if</span> featVec[axis] == value:</div><div class="line">			reducedFeatVec = featVec[:axis]</div><div class="line">			reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</div><div class="line">			retDataSet.append(reducedFeatVec)</div><div class="line">	<span class="keyword">return</span> retDataSet</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span>	//通过比较信息增益选择最佳划分属性</div><div class="line">	numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span></div><div class="line">	baseEntropy = calcShannonEnt(dataSet)</div><div class="line">	bestInfoGain = <span class="number">0.0</span>; bestFeature = <span class="number">-1</span></div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</div><div class="line">		featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">		uniqueVals = set(featList)</div><div class="line">		newEntropy = <span class="number">0.0</span></div><div class="line">		<span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">			subDataSet = splitDataSet(dataSet,i,value)</div><div class="line">			prob = len(subDataSet)/float(len(dataSet))</div><div class="line">			newEntropy += prob * calcShannonEnt(subDataSet)</div><div class="line">		infoGain = baseEntropy - newEntropy</div><div class="line">		<span class="keyword">if</span> (infoGain &gt; bestInfoGain):</div><div class="line">			bestInfoGain = infoGain</div><div class="line">			bestFeature = i</div><div class="line">	<span class="keyword">return</span> bestFeature</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span>	//选择出现次数最多的分类</div><div class="line">	classCount = &#123;&#125;</div><div class="line">	<span class="keyword">for</span> vote <span class="keyword">in</span> classList:</div><div class="line">		<span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.key():classCount[vote] = <span class="number">0</span></div><div class="line">		classCount[vote] += <span class="number">1</span></div><div class="line">	sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="keyword">True</span>)</div><div class="line">	<span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span>	//创建决策树</div><div class="line">	classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">	<span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</div><div class="line">		<span class="keyword">return</span> classList[<span class="number">0</span>]</div><div class="line">	<span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</div><div class="line">		<span class="keyword">return</span> majorityCnt(classList)</div><div class="line">	bestFeat = chooseBestFeatureToSplit(dataSet)</div><div class="line">	bestFeatLabel = labels[bestFeat]</div><div class="line">	myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</div><div class="line">	<span class="keyword">del</span>(labels[bestFeat])</div><div class="line">	featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</div><div class="line">	uniqueVals = set(featValues)</div><div class="line">	<span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</div><div class="line">		subLabels = labels[:]</div><div class="line">		myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels)</div><div class="line">	<span class="keyword">return</span> myTree</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span>	//使用决策树执行分类</div><div class="line">	firstStr = inputTree.keys()[<span class="number">0</span>]</div><div class="line">	secondDict = inputTree[firstStr]</div><div class="line">	featIndex = featLabels.index(firstStr)</div><div class="line">	<span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</div><div class="line">		<span class="keyword">if</span> testVec[featIndex] == key:</div><div class="line">			<span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:</div><div class="line">				classLabel = classify(secondDict[key],featLabels,testVec)</div><div class="line">			<span class="keyword">else</span>:</div><div class="line">				classLabel = secondDict[key]</div><div class="line">	<span class="keyword">return</span> classLabel</div></pre></td></tr></table></figure>
<p>可以看出，ID3决策树算法是一个递归过程，而在算法种有三种情况会导致递归返回：</p>
<ol>
<li><p>当前节点包含的样本全属于同一类别，无需划分。</p>
</li>
<li><p>属性集为空，或是所有样本在所有属性上的取值相同，无法划分。</p>
</li>
<li><p>当前节点包含的样本集合为空，不能划分。</p>
</li>
</ol>
<p>在第2种情况下，我们把当前节点标记为叶节点，并将其类别设定为该节点样本最多的类别；在第3种情况下，同样把当前节点标记为叶节点，但将其类别设定为其父节点所含样本最多的类别，注意这两种情形的处理实质不同：情形2是在利用当前节点的后验分布，而情形3则是把父节点的样本分布作为当前节点的先验分布。</p>
<p>总结一下：决策树算法就是通过信息增益不断递归寻找最好的划分方式，构建树的过程就好比“开枝散叶”，当有新的分类任务时可以很快的从树的“根”到“叶”找到最终的分类结果。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习实战-决策树算法（ID3）&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/03/27/hello-world/"/>
    <id>http://yoursite.com/2017/03/27/hello-world/</id>
    <published>2017-03-27T15:16:55.932Z</published>
    <updated>2017-03-27T15:16:55.932Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
